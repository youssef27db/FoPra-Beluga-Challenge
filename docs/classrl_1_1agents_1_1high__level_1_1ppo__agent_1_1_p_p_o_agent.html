<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.12.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>FoPra Beluga Challenge - Reinforcement Learning: rl.agents.high_level.ppo_agent.PPOAgent Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="clipboard.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<script type="text/javascript" src="darkmode_toggle.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">FoPra Beluga Challenge - Reinforcement Learning<span id="projectnumber">&#160;v1.0</span>
   </div>
   <div id="projectbrief">Deep Reinforcement Learning solution for the Beluga Challenge shipping container optimization problem using PPO and MCTS</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.12.0 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(0); });
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',false,false,'search.php','Search',true);
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){initNavTree('classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent.html',''); initResizable(true); });
/* @license-end */
</script>
<div id="doc-content">
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pub-attribs">Public Attributes</a> &#124;
<a href="classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent-members.html">List of all members</a>  </div>
  <div class="headertitle"><div class="title">rl.agents.high_level.ppo_agent.PPOAgent Class Reference</div></div>
</div><!--header-->
<div class="contents">

<p>Proximal Policy Optimization (PPO) agent implementation.  
 <a href="#details">More...</a></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-methods" name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a2a0c96183d523d54d8203e10dcb129a3" id="r_a2a0c96183d523d54d8203e10dcb129a3"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a2a0c96183d523d54d8203e10dcb129a3">__init__</a> (self, input_dims, n_actions, <a class="el" href="#a9a01dc3182b551836c5549a8c172b651">gamma</a>=0.99, alpha=0.0005, <a class="el" href="#a2e14dfc5da07b12391625092aad3a8da">gae_lambda</a>=0.95, <a class="el" href="#a6943862574b28a3371d19ee56455a544">policy_clip</a>=0.2, batch_size=128, N=1024, <a class="el" href="#a7ba79036643adb93f11d205be1dd0b5f">n_epochs</a>=5, model_name='ppo')</td></tr>
<tr class="memdesc:a2a0c96183d523d54d8203e10dcb129a3"><td class="mdescLeft">&#160;</td><td class="mdescRight">Initialize the PPO agent.  <br /></td></tr>
<tr class="separator:a2a0c96183d523d54d8203e10dcb129a3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7caa3ca2d12d84e7eab133650fa115c4" id="r_a7caa3ca2d12d84e7eab133650fa115c4"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a7caa3ca2d12d84e7eab133650fa115c4">remember</a> (self, state, action, probs, values, reward, done)</td></tr>
<tr class="memdesc:a7caa3ca2d12d84e7eab133650fa115c4"><td class="mdescLeft">&#160;</td><td class="mdescRight">Store an experience in the agent's memory.  <br /></td></tr>
<tr class="separator:a7caa3ca2d12d84e7eab133650fa115c4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a470678495edd7cd31b9c58d6d95ed7c0" id="r_a470678495edd7cd31b9c58d6d95ed7c0"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a470678495edd7cd31b9c58d6d95ed7c0">save_models</a> (self)</td></tr>
<tr class="memdesc:a470678495edd7cd31b9c58d6d95ed7c0"><td class="mdescLeft">&#160;</td><td class="mdescRight">Save both actor and critic model checkpoints.  <br /></td></tr>
<tr class="separator:a470678495edd7cd31b9c58d6d95ed7c0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a661abfa90d86386f305e8f947f0322f9" id="r_a661abfa90d86386f305e8f947f0322f9"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a661abfa90d86386f305e8f947f0322f9">load_models</a> (self)</td></tr>
<tr class="memdesc:a661abfa90d86386f305e8f947f0322f9"><td class="mdescLeft">&#160;</td><td class="mdescRight">Load both actor and critic model checkpoints.  <br /></td></tr>
<tr class="separator:a661abfa90d86386f305e8f947f0322f9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8992b4ff91e79bc53277932481061250" id="r_a8992b4ff91e79bc53277932481061250"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a8992b4ff91e79bc53277932481061250">choose_action</a> (self, observation)</td></tr>
<tr class="memdesc:a8992b4ff91e79bc53277932481061250"><td class="mdescLeft">&#160;</td><td class="mdescRight">Choose an action based on the current observation.  <br /></td></tr>
<tr class="separator:a8992b4ff91e79bc53277932481061250"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7899457347773c0e1ef284fe4cd91d19" id="r_a7899457347773c0e1ef284fe4cd91d19"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a7899457347773c0e1ef284fe4cd91d19">learn</a> (self)</td></tr>
<tr class="memdesc:a7899457347773c0e1ef284fe4cd91d19"><td class="mdescLeft">&#160;</td><td class="mdescRight">Perform PPO learning update using stored experiences.  <br /></td></tr>
<tr class="separator:a7899457347773c0e1ef284fe4cd91d19"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-attribs" name="pub-attribs"></a>
Public Attributes</h2></td></tr>
<tr class="memitem:a9a01dc3182b551836c5549a8c172b651" id="r_a9a01dc3182b551836c5549a8c172b651"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a9a01dc3182b551836c5549a8c172b651">gamma</a> = gamma</td></tr>
<tr class="separator:a9a01dc3182b551836c5549a8c172b651"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6943862574b28a3371d19ee56455a544" id="r_a6943862574b28a3371d19ee56455a544"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a6943862574b28a3371d19ee56455a544">policy_clip</a> = policy_clip</td></tr>
<tr class="separator:a6943862574b28a3371d19ee56455a544"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7ba79036643adb93f11d205be1dd0b5f" id="r_a7ba79036643adb93f11d205be1dd0b5f"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a7ba79036643adb93f11d205be1dd0b5f">n_epochs</a> = n_epochs</td></tr>
<tr class="separator:a7ba79036643adb93f11d205be1dd0b5f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2e14dfc5da07b12391625092aad3a8da" id="r_a2e14dfc5da07b12391625092aad3a8da"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a2e14dfc5da07b12391625092aad3a8da">gae_lambda</a> = gae_lambda</td></tr>
<tr class="separator:a2e14dfc5da07b12391625092aad3a8da"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a30d5505efa99710341c579de4f3c7c03" id="r_a30d5505efa99710341c579de4f3c7c03"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a30d5505efa99710341c579de4f3c7c03">actor</a> = <a class="el" href="classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_actor_network.html">ActorNetwork</a>(n_actions, input_dims, alpha, name = model_name)</td></tr>
<tr class="separator:a30d5505efa99710341c579de4f3c7c03"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a476d79433aee6877dcf08d4985a8b798" id="r_a476d79433aee6877dcf08d4985a8b798"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a476d79433aee6877dcf08d4985a8b798">critic</a> = <a class="el" href="classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_critic_network.html">CriticNetwork</a>(input_dims, alpha, name = model_name)</td></tr>
<tr class="separator:a476d79433aee6877dcf08d4985a8b798"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa806d28253ea490fc8b519af229bec28" id="r_aa806d28253ea490fc8b519af229bec28"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#aa806d28253ea490fc8b519af229bec28">memory</a> = <a class="el" href="classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory.html">PPOMemory</a>(batch_size)</td></tr>
<tr class="separator:aa806d28253ea490fc8b519af229bec28"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>Proximal Policy Optimization (PPO) agent implementation. </p>
<p>This class implements the PPO algorithm for reinforcement learning. It uses an actor-critic architecture with clipped probability ratios to ensure stable policy updates. </p>
</div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="a2a0c96183d523d54d8203e10dcb129a3" name="a2a0c96183d523d54d8203e10dcb129a3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2a0c96183d523d54d8203e10dcb129a3">&#9670;&#160;</a></span>__init__()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rl.agents.high_level.ppo_agent.PPOAgent.__init__ </td>
          <td>(</td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>self</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>input_dims</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>n_actions</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>gamma</em></span><span class="paramdefsep"> = </span><span class="paramdefval">0.99</span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>alpha</em></span><span class="paramdefsep"> = </span><span class="paramdefval">0.0005</span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>gae_lambda</em></span><span class="paramdefsep"> = </span><span class="paramdefval">0.95</span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>policy_clip</em></span><span class="paramdefsep"> = </span><span class="paramdefval">0.2</span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>batch_size</em></span><span class="paramdefsep"> = </span><span class="paramdefval">128</span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>N</em></span><span class="paramdefsep"> = </span><span class="paramdefval">1024</span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>n_epochs</em></span><span class="paramdefsep"> = </span><span class="paramdefval">5</span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>model_name</em></span><span class="paramdefsep"> = </span><span class="paramdefval">'ppo'</span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Initialize the PPO agent. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">input_dims</td><td>Dimension of the state space </td></tr>
    <tr><td class="paramname">n_actions</td><td>Number of possible actions </td></tr>
    <tr><td class="paramname">gamma</td><td>Discount factor for future rewards </td></tr>
    <tr><td class="paramname">alpha</td><td>Learning rate for both actor and critic networks </td></tr>
    <tr><td class="paramname">gae_lambda</td><td>Lambda parameter for Generalized Advantage Estimation </td></tr>
    <tr><td class="paramname">policy_clip</td><td>Clipping parameter for PPO objective </td></tr>
    <tr><td class="paramname">batch_size</td><td>Size of training batches </td></tr>
    <tr><td class="paramname">N</td><td>Number of steps to collect before learning </td></tr>
    <tr><td class="paramname">n_epochs</td><td>Number of training epochs per learning step </td></tr>
    <tr><td class="paramname">model_name</td><td>Name for saving/loading model checkpoints </td></tr>
  </table>
  </dd>
</dl>
<div class="fragment"><div class="line"><span class="lineno">  195</span>                 policy_clip=0.2, batch_size=128, N=1024, n_epochs=5, model_name =<span class="stringliteral">&#39;ppo&#39;</span>):</div>
<div class="line"><span class="lineno">  196</span>        <span class="stringliteral">&quot;&quot;&quot;!</span></div>
<div class="line"><span class="lineno">  197</span><span class="stringliteral">        @brief Initialize the PPO agent</span></div>
<div class="line"><span class="lineno">  198</span><span class="stringliteral">        @param input_dims Dimension of the state space</span></div>
<div class="line"><span class="lineno">  199</span><span class="stringliteral">        @param n_actions Number of possible actions</span></div>
<div class="line"><span class="lineno">  200</span><span class="stringliteral">        @param gamma Discount factor for future rewards</span></div>
<div class="line"><span class="lineno">  201</span><span class="stringliteral">        @param alpha Learning rate for both actor and critic networks</span></div>
<div class="line"><span class="lineno">  202</span><span class="stringliteral">        @param gae_lambda Lambda parameter for Generalized Advantage Estimation</span></div>
<div class="line"><span class="lineno">  203</span><span class="stringliteral">        @param policy_clip Clipping parameter for PPO objective</span></div>
<div class="line"><span class="lineno">  204</span><span class="stringliteral">        @param batch_size Size of training batches</span></div>
<div class="line"><span class="lineno">  205</span><span class="stringliteral">        @param N Number of steps to collect before learning</span></div>
<div class="line"><span class="lineno">  206</span><span class="stringliteral">        @param n_epochs Number of training epochs per learning step</span></div>
<div class="line"><span class="lineno">  207</span><span class="stringliteral">        @param model_name Name for saving/loading model checkpoints</span></div>
<div class="line"><span class="lineno">  208</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  209</span>        self.gamma = gamma</div>
<div class="line"><span class="lineno">  210</span>        self.policy_clip = policy_clip</div>
<div class="line"><span class="lineno">  211</span>        self.n_epochs = n_epochs</div>
<div class="line"><span class="lineno">  212</span>        self.gae_lambda = gae_lambda</div>
<div class="line"><span class="lineno">  213</span> </div>
<div class="line"><span class="lineno">  214</span>        self.actor = ActorNetwork(n_actions, input_dims, alpha, name = model_name)</div>
<div class="line"><span class="lineno">  215</span>        self.critic = CriticNetwork(input_dims, alpha, name = model_name)</div>
<div class="line"><span class="lineno">  216</span>        self.memory = PPOMemory(batch_size)</div>
<div class="line"><span class="lineno">  217</span> </div>
</div><!-- fragment -->
</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="a8992b4ff91e79bc53277932481061250" name="a8992b4ff91e79bc53277932481061250"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8992b4ff91e79bc53277932481061250">&#9670;&#160;</a></span>choose_action()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rl.agents.high_level.ppo_agent.PPOAgent.choose_action </td>
          <td>(</td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>self</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>observation</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Choose an action based on the current observation. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">observation</td><td>Current state observation </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Tuple of (action, log_probability, value_estimate, action_distribution) </dd></dl>
<div class="fragment"><div class="line"><span class="lineno">  246</span>    <span class="keyword">def </span>choose_action(self, observation):</div>
<div class="line"><span class="lineno">  247</span>        <span class="stringliteral">&quot;&quot;&quot;!</span></div>
<div class="line"><span class="lineno">  248</span><span class="stringliteral">        @brief Choose an action based on the current observation</span></div>
<div class="line"><span class="lineno">  249</span><span class="stringliteral">        @param observation Current state observation</span></div>
<div class="line"><span class="lineno">  250</span><span class="stringliteral">        @return Tuple of (action, log_probability, value_estimate, action_distribution)</span></div>
<div class="line"><span class="lineno">  251</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  252</span>        <span class="comment"># Convert observation to tensor</span></div>
<div class="line"><span class="lineno">  253</span>        state = T.tensor(observation, dtype=T.float).to(self.actor.device)</div>
<div class="line"><span class="lineno">  254</span> </div>
<div class="line"><span class="lineno">  255</span>        dist = self.actor(state)</div>
<div class="line"><span class="lineno">  256</span>        value = self.critic(state)</div>
<div class="line"><span class="lineno">  257</span>        action = dist.sample()</div>
<div class="line"><span class="lineno">  258</span> </div>
<div class="line"><span class="lineno">  259</span>        probs = T.squeeze(dist.log_prob(action)).item()</div>
<div class="line"><span class="lineno">  260</span>        action = T.squeeze(action).item()</div>
<div class="line"><span class="lineno">  261</span>        value = T.squeeze(value).item()</div>
<div class="line"><span class="lineno">  262</span> </div>
<div class="line"><span class="lineno">  263</span>        <span class="keywordflow">return</span> action, probs, value, dist</div>
<div class="line"><span class="lineno">  264</span> </div>
<div class="line"><span class="lineno">  265</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a7899457347773c0e1ef284fe4cd91d19" name="a7899457347773c0e1ef284fe4cd91d19"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7899457347773c0e1ef284fe4cd91d19">&#9670;&#160;</a></span>learn()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rl.agents.high_level.ppo_agent.PPOAgent.learn </td>
          <td>(</td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>self</em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Perform PPO learning update using stored experiences. </p>
<p>Implements the PPO algorithm with clipped probability ratios and Generalized Advantage Estimation (GAE). Updates both actor and critic networks for multiple epochs. </p>
<div class="fragment"><div class="line"><span class="lineno">  266</span>    <span class="keyword">def </span>learn(self):</div>
<div class="line"><span class="lineno">  267</span>        <span class="stringliteral">&quot;&quot;&quot;!</span></div>
<div class="line"><span class="lineno">  268</span><span class="stringliteral">        @brief Perform PPO learning update using stored experiences</span></div>
<div class="line"><span class="lineno">  269</span><span class="stringliteral">        </span></div>
<div class="line"><span class="lineno">  270</span><span class="stringliteral">        Implements the PPO algorithm with clipped probability ratios and </span></div>
<div class="line"><span class="lineno">  271</span><span class="stringliteral">        Generalized Advantage Estimation (GAE). Updates both actor and </span></div>
<div class="line"><span class="lineno">  272</span><span class="stringliteral">        critic networks for multiple epochs.</span></div>
<div class="line"><span class="lineno">  273</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  274</span>        <span class="keywordflow">for</span> _ <span class="keywordflow">in</span> range(self.n_epochs):</div>
<div class="line"><span class="lineno">  275</span>            state_arr, action_arr, old_probs_arr, values_arr,\</div>
<div class="line"><span class="lineno">  276</span>            reward_arr, done_arr, batches = self.memory.generate_batches()</div>
<div class="line"><span class="lineno">  277</span> </div>
<div class="line"><span class="lineno">  278</span>            values = values_arr</div>
<div class="line"><span class="lineno">  279</span>            advantages = np.zeros(len(reward_arr), dtype=np.float32)</div>
<div class="line"><span class="lineno">  280</span> </div>
<div class="line"><span class="lineno">  281</span>            <span class="comment"># Calculate advantages using GAE</span></div>
<div class="line"><span class="lineno">  282</span>            <span class="keywordflow">for</span> t <span class="keywordflow">in</span> range(len(reward_arr)-1):</div>
<div class="line"><span class="lineno">  283</span>                discount = 1</div>
<div class="line"><span class="lineno">  284</span>                a_t = 0</div>
<div class="line"><span class="lineno">  285</span>                <span class="keywordflow">for</span> k <span class="keywordflow">in</span> range(t, len(reward_arr)-1):</div>
<div class="line"><span class="lineno">  286</span>                   a_t += discount*(reward_arr[k] + self.gamma*values[k+1]*(1-int(done_arr[k])) - values[k])</div>
<div class="line"><span class="lineno">  287</span>                   discount *= self.gamma*self.gae_lambda</div>
<div class="line"><span class="lineno">  288</span>                advantages[t] = a_t</div>
<div class="line"><span class="lineno">  289</span> </div>
<div class="line"><span class="lineno">  290</span>            advantage = T.tensor(advantages).to(self.actor.device)</div>
<div class="line"><span class="lineno">  291</span>            values = T.tensor(values).to(self.actor.device)</div>
<div class="line"><span class="lineno">  292</span> </div>
<div class="line"><span class="lineno">  293</span>            <span class="comment"># Train on each batch</span></div>
<div class="line"><span class="lineno">  294</span>            <span class="keywordflow">for</span> batch <span class="keywordflow">in</span> batches:</div>
<div class="line"><span class="lineno">  295</span>                states = T.tensor(state_arr[batch], dtype=T.float).to(self.actor.device)</div>
<div class="line"><span class="lineno">  296</span>                old_probs = T.tensor(old_probs_arr[batch]).to(self.actor.device)</div>
<div class="line"><span class="lineno">  297</span>                actions = T.tensor(action_arr[batch]).to(self.actor.device)</div>
<div class="line"><span class="lineno">  298</span> </div>
<div class="line"><span class="lineno">  299</span>                dist = self.actor(states)</div>
<div class="line"><span class="lineno">  300</span>                critic_value = self.critic(states)</div>
<div class="line"><span class="lineno">  301</span> </div>
<div class="line"><span class="lineno">  302</span>                critic_value = T.squeeze(critic_value)</div>
<div class="line"><span class="lineno">  303</span> </div>
<div class="line"><span class="lineno">  304</span>                <span class="comment"># Calculate probability ratio and apply clipping</span></div>
<div class="line"><span class="lineno">  305</span>                new_probs = dist.log_prob(actions)</div>
<div class="line"><span class="lineno">  306</span>                prob_ratio = new_probs.exp() / old_probs.exp()</div>
<div class="line"><span class="lineno">  307</span>                weighted_probs = advantage[batch] * prob_ratio</div>
<div class="line"><span class="lineno">  308</span>                weighted_clipped_probs = T.clamp(prob_ratio, 1-self.policy_clip, 1+self.policy_clip) * advantage[batch]</div>
<div class="line"><span class="lineno">  309</span>                actor_loss = -T.min(weighted_probs, weighted_clipped_probs).mean()</div>
<div class="line"><span class="lineno">  310</span> </div>
<div class="line"><span class="lineno">  311</span>                <span class="comment"># Calculate critic loss</span></div>
<div class="line"><span class="lineno">  312</span>                returns = advantage[batch] + values[batch]</div>
<div class="line"><span class="lineno">  313</span>                critic_loss = (returns-critic_value)**2</div>
<div class="line"><span class="lineno">  314</span>                critic_loss = critic_loss.mean()</div>
<div class="line"><span class="lineno">  315</span> </div>
<div class="line"><span class="lineno">  316</span>                <span class="comment"># Combined loss and backpropagation</span></div>
<div class="line"><span class="lineno">  317</span>                total_loss = actor_loss + 0.5*critic_loss</div>
<div class="line"><span class="lineno">  318</span>                self.actor.optimizer.zero_grad()</div>
<div class="line"><span class="lineno">  319</span>                self.critic.optimizer.zero_grad()</div>
<div class="line"><span class="lineno">  320</span>                total_loss.backward()</div>
<div class="line"><span class="lineno">  321</span>                self.actor.optimizer.step()</div>
<div class="line"><span class="lineno">  322</span>                self.critic.optimizer.step()</div>
<div class="line"><span class="lineno">  323</span> </div>
<div class="line"><span class="lineno">  324</span>        self.memory.clear_memory()</div>
<div class="line"><span class="lineno">  325</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a661abfa90d86386f305e8f947f0322f9" name="a661abfa90d86386f305e8f947f0322f9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a661abfa90d86386f305e8f947f0322f9">&#9670;&#160;</a></span>load_models()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rl.agents.high_level.ppo_agent.PPOAgent.load_models </td>
          <td>(</td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>self</em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Load both actor and critic model checkpoints. </p>
<div class="fragment"><div class="line"><span class="lineno">  238</span>    <span class="keyword">def </span>load_models(self):</div>
<div class="line"><span class="lineno">  239</span>        <span class="stringliteral">&quot;&quot;&quot;!</span></div>
<div class="line"><span class="lineno">  240</span><span class="stringliteral">        @brief Load both actor and critic model checkpoints</span></div>
<div class="line"><span class="lineno">  241</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  242</span>        print(<span class="stringliteral">&#39;... loading models ...&#39;</span>)</div>
<div class="line"><span class="lineno">  243</span>        self.actor.load_checkpoint()</div>
<div class="line"><span class="lineno">  244</span>        self.critic.load_checkpoint()</div>
<div class="line"><span class="lineno">  245</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a7caa3ca2d12d84e7eab133650fa115c4" name="a7caa3ca2d12d84e7eab133650fa115c4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7caa3ca2d12d84e7eab133650fa115c4">&#9670;&#160;</a></span>remember()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rl.agents.high_level.ppo_agent.PPOAgent.remember </td>
          <td>(</td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>self</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>state</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>action</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>probs</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>values</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>reward</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>done</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Store an experience in the agent's memory. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">state</td><td>Current state observation </td></tr>
    <tr><td class="paramname">action</td><td>Action taken </td></tr>
    <tr><td class="paramname">probs</td><td>Log probability of the action </td></tr>
    <tr><td class="paramname">values</td><td>Value estimate for the state </td></tr>
    <tr><td class="paramname">reward</td><td>Reward received </td></tr>
    <tr><td class="paramname">done</td><td>Whether the episode is finished </td></tr>
  </table>
  </dd>
</dl>
<div class="fragment"><div class="line"><span class="lineno">  218</span>    <span class="keyword">def </span>remember(self, state, action, probs, values, reward, done):</div>
<div class="line"><span class="lineno">  219</span>        <span class="stringliteral">&quot;&quot;&quot;!</span></div>
<div class="line"><span class="lineno">  220</span><span class="stringliteral">        @brief Store an experience in the agent&#39;s memory</span></div>
<div class="line"><span class="lineno">  221</span><span class="stringliteral">        @param state Current state observation</span></div>
<div class="line"><span class="lineno">  222</span><span class="stringliteral">        @param action Action taken</span></div>
<div class="line"><span class="lineno">  223</span><span class="stringliteral">        @param probs Log probability of the action</span></div>
<div class="line"><span class="lineno">  224</span><span class="stringliteral">        @param values Value estimate for the state</span></div>
<div class="line"><span class="lineno">  225</span><span class="stringliteral">        @param reward Reward received</span></div>
<div class="line"><span class="lineno">  226</span><span class="stringliteral">        @param done Whether the episode is finished</span></div>
<div class="line"><span class="lineno">  227</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  228</span>        self.memory.store_memory(state, action, probs, values, reward, done)</div>
<div class="line"><span class="lineno">  229</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a470678495edd7cd31b9c58d6d95ed7c0" name="a470678495edd7cd31b9c58d6d95ed7c0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a470678495edd7cd31b9c58d6d95ed7c0">&#9670;&#160;</a></span>save_models()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rl.agents.high_level.ppo_agent.PPOAgent.save_models </td>
          <td>(</td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>self</em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Save both actor and critic model checkpoints. </p>
<div class="fragment"><div class="line"><span class="lineno">  230</span>    <span class="keyword">def </span>save_models(self):</div>
<div class="line"><span class="lineno">  231</span>        <span class="stringliteral">&quot;&quot;&quot;!</span></div>
<div class="line"><span class="lineno">  232</span><span class="stringliteral">        @brief Save both actor and critic model checkpoints</span></div>
<div class="line"><span class="lineno">  233</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  234</span>        print(<span class="stringliteral">&#39;... saving models ...&#39;</span>)</div>
<div class="line"><span class="lineno">  235</span>        self.actor.save_checkpoint()</div>
<div class="line"><span class="lineno">  236</span>        self.critic.save_checkpoint()</div>
<div class="line"><span class="lineno">  237</span> </div>
</div><!-- fragment -->
</div>
</div>
<h2 class="groupheader">Member Data Documentation</h2>
<a id="a30d5505efa99710341c579de4f3c7c03" name="a30d5505efa99710341c579de4f3c7c03"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a30d5505efa99710341c579de4f3c7c03">&#9670;&#160;</a></span>actor</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rl.agents.high_level.ppo_agent.PPOAgent.actor = <a class="el" href="classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_actor_network.html">ActorNetwork</a>(n_actions, input_dims, alpha, name = model_name)</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a476d79433aee6877dcf08d4985a8b798" name="a476d79433aee6877dcf08d4985a8b798"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a476d79433aee6877dcf08d4985a8b798">&#9670;&#160;</a></span>critic</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rl.agents.high_level.ppo_agent.PPOAgent.critic = <a class="el" href="classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_critic_network.html">CriticNetwork</a>(input_dims, alpha, name = model_name)</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a2e14dfc5da07b12391625092aad3a8da" name="a2e14dfc5da07b12391625092aad3a8da"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2e14dfc5da07b12391625092aad3a8da">&#9670;&#160;</a></span>gae_lambda</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rl.agents.high_level.ppo_agent.PPOAgent.gae_lambda = gae_lambda</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a9a01dc3182b551836c5549a8c172b651" name="a9a01dc3182b551836c5549a8c172b651"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9a01dc3182b551836c5549a8c172b651">&#9670;&#160;</a></span>gamma</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rl.agents.high_level.ppo_agent.PPOAgent.gamma = gamma</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="aa806d28253ea490fc8b519af229bec28" name="aa806d28253ea490fc8b519af229bec28"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa806d28253ea490fc8b519af229bec28">&#9670;&#160;</a></span>memory</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rl.agents.high_level.ppo_agent.PPOAgent.memory = <a class="el" href="classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory.html">PPOMemory</a>(batch_size)</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a7ba79036643adb93f11d205be1dd0b5f" name="a7ba79036643adb93f11d205be1dd0b5f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7ba79036643adb93f11d205be1dd0b5f">&#9670;&#160;</a></span>n_epochs</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rl.agents.high_level.ppo_agent.PPOAgent.n_epochs = n_epochs</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a6943862574b28a3371d19ee56455a544" name="a6943862574b28a3371d19ee56455a544"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6943862574b28a3371d19ee56455a544">&#9670;&#160;</a></span>policy_clip</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">rl.agents.high_level.ppo_agent.PPOAgent.policy_clip = policy_clip</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<hr/>The documentation for this class was generated from the following file:<ul>
<li>rl/agents/high_level/<b>ppo_agent.py</b></li>
</ul>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="namespacerl.html">rl</a></li><li class="navelem"><a class="el" href="namespacerl_1_1agents.html">agents</a></li><li class="navelem"><a class="el" href="namespacerl_1_1agents_1_1high__level.html">high_level</a></li><li class="navelem"><a class="el" href="namespacerl_1_1agents_1_1high__level_1_1ppo__agent.html">ppo_agent</a></li><li class="navelem"><a class="el" href="classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent.html">PPOAgent</a></li>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.12.0 </li>
  </ul>
</div>
</body>
</html>
