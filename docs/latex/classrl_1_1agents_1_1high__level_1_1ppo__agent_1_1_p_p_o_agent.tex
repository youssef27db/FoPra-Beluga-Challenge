\doxysection{rl.\+agents.\+high\+\_\+level.\+ppo\+\_\+agent.\+PPOAgent Class Reference}
\hypertarget{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent}{}\label{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent}\index{rl.agents.high\_level.ppo\_agent.PPOAgent@{rl.agents.high\_level.ppo\_agent.PPOAgent}}


Proximal Policy Optimization (PPO) agent implementation.  


\doxysubsubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a2a0c96183d523d54d8203e10dcb129a3}{\+\_\+\+\_\+init\+\_\+\+\_\+}} (self, input\+\_\+dims, n\+\_\+actions, \mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a9a01dc3182b551836c5549a8c172b651}{gamma}}=0.\+99, alpha=0.\+0005, \mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a2e14dfc5da07b12391625092aad3a8da}{gae\+\_\+lambda}}=0.\+95, \mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a6943862574b28a3371d19ee56455a544}{policy\+\_\+clip}}=0.\+2, batch\+\_\+size=128, N=1024, \mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a7ba79036643adb93f11d205be1dd0b5f}{n\+\_\+epochs}}=5, model\+\_\+name=\textquotesingle{}ppo\textquotesingle{})
\begin{DoxyCompactList}\small\item\em Initialize the PPO agent. \end{DoxyCompactList}\item 
\mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a7caa3ca2d12d84e7eab133650fa115c4}{remember}} (self, state, action, probs, values, reward, done)
\begin{DoxyCompactList}\small\item\em Store an experience in the agent\textquotesingle{}s memory. \end{DoxyCompactList}\item 
\mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a470678495edd7cd31b9c58d6d95ed7c0}{save\+\_\+models}} (self)
\begin{DoxyCompactList}\small\item\em Save both actor and critic model checkpoints. \end{DoxyCompactList}\item 
\mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a661abfa90d86386f305e8f947f0322f9}{load\+\_\+models}} (self)
\begin{DoxyCompactList}\small\item\em Load both actor and critic model checkpoints. \end{DoxyCompactList}\item 
\mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a8992b4ff91e79bc53277932481061250}{choose\+\_\+action}} (self, observation)
\begin{DoxyCompactList}\small\item\em Choose an action based on the current observation. \end{DoxyCompactList}\item 
\mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a7899457347773c0e1ef284fe4cd91d19}{learn}} (self)
\begin{DoxyCompactList}\small\item\em Perform PPO learning update using stored experiences. \end{DoxyCompactList}\end{DoxyCompactItemize}
\doxysubsubsection*{Public Attributes}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a9a01dc3182b551836c5549a8c172b651}{gamma}} = gamma
\item 
\mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a6943862574b28a3371d19ee56455a544}{policy\+\_\+clip}} = policy\+\_\+clip
\item 
\mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a7ba79036643adb93f11d205be1dd0b5f}{n\+\_\+epochs}} = n\+\_\+epochs
\item 
\mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a2e14dfc5da07b12391625092aad3a8da}{gae\+\_\+lambda}} = gae\+\_\+lambda
\item 
\mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a30d5505efa99710341c579de4f3c7c03}{actor}} = \mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_actor_network}{Actor\+Network}}(n\+\_\+actions, input\+\_\+dims, alpha, name = model\+\_\+name)
\item 
\mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a476d79433aee6877dcf08d4985a8b798}{critic}} = \mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_critic_network}{Critic\+Network}}(input\+\_\+dims, alpha, name = model\+\_\+name)
\item 
\mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_aa806d28253ea490fc8b519af229bec28}{memory}} = \mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory}{PPOMemory}}(batch\+\_\+size)
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
Proximal Policy Optimization (PPO) agent implementation. 

This class implements the PPO algorithm for reinforcement learning. It uses an actor-\/critic architecture with clipped probability ratios to ensure stable policy updates. 

\doxysubsection{Constructor \& Destructor Documentation}
\Hypertarget{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a2a0c96183d523d54d8203e10dcb129a3}\index{rl.agents.high\_level.ppo\_agent.PPOAgent@{rl.agents.high\_level.ppo\_agent.PPOAgent}!\_\_init\_\_@{\_\_init\_\_}}
\index{\_\_init\_\_@{\_\_init\_\_}!rl.agents.high\_level.ppo\_agent.PPOAgent@{rl.agents.high\_level.ppo\_agent.PPOAgent}}
\doxysubsubsection{\texorpdfstring{\_\_init\_\_()}{\_\_init\_\_()}}
{\footnotesize\ttfamily \label{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a2a0c96183d523d54d8203e10dcb129a3} 
rl.\+agents.\+high\+\_\+level.\+ppo\+\_\+agent.\+PPOAgent.\+\_\+\+\_\+init\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{input\+\_\+dims}{, }\item[{}]{n\+\_\+actions}{, }\item[{}]{gamma}{ = {\ttfamily 0.99}, }\item[{}]{alpha}{ = {\ttfamily 0.0005}, }\item[{}]{gae\+\_\+lambda}{ = {\ttfamily 0.95}, }\item[{}]{policy\+\_\+clip}{ = {\ttfamily 0.2}, }\item[{}]{batch\+\_\+size}{ = {\ttfamily 128}, }\item[{}]{N}{ = {\ttfamily 1024}, }\item[{}]{n\+\_\+epochs}{ = {\ttfamily 5}, }\item[{}]{model\+\_\+name}{ = {\ttfamily \textquotesingle{}ppo\textquotesingle{}}}\end{DoxyParamCaption})}



Initialize the PPO agent. 


\begin{DoxyParams}{Parameters}
{\em input\+\_\+dims} & Dimension of the state space \\
\hline
{\em n\+\_\+actions} & Number of possible actions \\
\hline
{\em gamma} & Discount factor for future rewards \\
\hline
{\em alpha} & Learning rate for both actor and critic networks \\
\hline
{\em gae\+\_\+lambda} & Lambda parameter for Generalized Advantage Estimation \\
\hline
{\em policy\+\_\+clip} & Clipping parameter for PPO objective \\
\hline
{\em batch\+\_\+size} & Size of training batches \\
\hline
{\em N} & Number of steps to collect before learning \\
\hline
{\em n\+\_\+epochs} & Number of training epochs per learning step \\
\hline
{\em model\+\_\+name} & Name for saving/loading model checkpoints \\
\hline
\end{DoxyParams}

\begin{DoxyCode}{0}
\DoxyCodeLine{00195\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ policy\_clip=0.2,\ batch\_size=128,\ N=1024,\ n\_epochs=5,\ model\_name\ =\textcolor{stringliteral}{'ppo'}): }
\DoxyCodeLine{00196\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}! }}
\DoxyCodeLine{00197\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @brief\ Initialize\ the\ PPO\ agent }}
\DoxyCodeLine{00198\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @param\ input\_dims\ Dimension\ of\ the\ state\ space }}
\DoxyCodeLine{00199\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @param\ n\_actions\ Number\ of\ possible\ actions }}
\DoxyCodeLine{00200\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @param\ gamma\ Discount\ factor\ for\ future\ rewards }}
\DoxyCodeLine{00201\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @param\ alpha\ Learning\ rate\ for\ both\ actor\ and\ critic\ networks }}
\DoxyCodeLine{00202\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @param\ gae\_lambda\ Lambda\ parameter\ for\ Generalized\ Advantage\ Estimation }}
\DoxyCodeLine{00203\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @param\ policy\_clip\ Clipping\ parameter\ for\ PPO\ objective }}
\DoxyCodeLine{00204\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @param\ batch\_size\ Size\ of\ training\ batches }}
\DoxyCodeLine{00205\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @param\ N\ Number\ of\ steps\ to\ collect\ before\ learning }}
\DoxyCodeLine{00206\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @param\ n\_epochs\ Number\ of\ training\ epochs\ per\ learning\ step }}
\DoxyCodeLine{00207\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @param\ model\_name\ Name\ for\ saving/loading\ model\ checkpoints }}
\DoxyCodeLine{00208\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}} }
\DoxyCodeLine{00209\ \ \ \ \ \ \ \ \ self.gamma\ =\ gamma }
\DoxyCodeLine{00210\ \ \ \ \ \ \ \ \ self.policy\_clip\ =\ policy\_clip }
\DoxyCodeLine{00211\ \ \ \ \ \ \ \ \ self.n\_epochs\ =\ n\_epochs }
\DoxyCodeLine{00212\ \ \ \ \ \ \ \ \ self.gae\_lambda\ =\ gae\_lambda }
\DoxyCodeLine{00213\  }
\DoxyCodeLine{00214\ \ \ \ \ \ \ \ \ self.actor\ =\ ActorNetwork(n\_actions,\ input\_dims,\ alpha,\ name\ =\ model\_name) }
\DoxyCodeLine{00215\ \ \ \ \ \ \ \ \ self.critic\ =\ CriticNetwork(input\_dims,\ alpha,\ name\ =\ model\_name) }
\DoxyCodeLine{00216\ \ \ \ \ \ \ \ \ self.memory\ =\ PPOMemory(batch\_size) }
\DoxyCodeLine{00217\  }

\end{DoxyCode}


\doxysubsection{Member Function Documentation}
\Hypertarget{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a8992b4ff91e79bc53277932481061250}\index{rl.agents.high\_level.ppo\_agent.PPOAgent@{rl.agents.high\_level.ppo\_agent.PPOAgent}!choose\_action@{choose\_action}}
\index{choose\_action@{choose\_action}!rl.agents.high\_level.ppo\_agent.PPOAgent@{rl.agents.high\_level.ppo\_agent.PPOAgent}}
\doxysubsubsection{\texorpdfstring{choose\_action()}{choose\_action()}}
{\footnotesize\ttfamily \label{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a8992b4ff91e79bc53277932481061250} 
rl.\+agents.\+high\+\_\+level.\+ppo\+\_\+agent.\+PPOAgent.\+choose\+\_\+action (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{observation}{}\end{DoxyParamCaption})}



Choose an action based on the current observation. 


\begin{DoxyParams}{Parameters}
{\em observation} & Current state observation \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
Tuple of (action, log\+\_\+probability, value\+\_\+estimate, action\+\_\+distribution) 
\end{DoxyReturn}

\begin{DoxyCode}{0}
\DoxyCodeLine{00246\ \ \ \ \ \textcolor{keyword}{def\ }choose\_action(self,\ observation): }
\DoxyCodeLine{00247\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}! }}
\DoxyCodeLine{00248\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @brief\ Choose\ an\ action\ based\ on\ the\ current\ observation }}
\DoxyCodeLine{00249\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @param\ observation\ Current\ state\ observation }}
\DoxyCodeLine{00250\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @return\ Tuple\ of\ (action,\ log\_probability,\ value\_estimate,\ action\_distribution) }}
\DoxyCodeLine{00251\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}} }
\DoxyCodeLine{00252\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Convert\ observation\ to\ tensor }}
\DoxyCodeLine{00253\ \ \ \ \ \ \ \ \ state\ =\ T.tensor(observation,\ dtype=T.float).to(self.actor.device) }
\DoxyCodeLine{00254\  }
\DoxyCodeLine{00255\ \ \ \ \ \ \ \ \ dist\ =\ self.actor(state) }
\DoxyCodeLine{00256\ \ \ \ \ \ \ \ \ value\ =\ self.critic(state) }
\DoxyCodeLine{00257\ \ \ \ \ \ \ \ \ action\ =\ dist.sample() }
\DoxyCodeLine{00258\  }
\DoxyCodeLine{00259\ \ \ \ \ \ \ \ \ probs\ =\ T.squeeze(dist.log\_prob(action)).item() }
\DoxyCodeLine{00260\ \ \ \ \ \ \ \ \ action\ =\ T.squeeze(action).item() }
\DoxyCodeLine{00261\ \ \ \ \ \ \ \ \ value\ =\ T.squeeze(value).item() }
\DoxyCodeLine{00262\  }
\DoxyCodeLine{00263\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ action,\ probs,\ value,\ dist }
\DoxyCodeLine{00264\  }
\DoxyCodeLine{00265\  }

\end{DoxyCode}
\Hypertarget{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a7899457347773c0e1ef284fe4cd91d19}\index{rl.agents.high\_level.ppo\_agent.PPOAgent@{rl.agents.high\_level.ppo\_agent.PPOAgent}!learn@{learn}}
\index{learn@{learn}!rl.agents.high\_level.ppo\_agent.PPOAgent@{rl.agents.high\_level.ppo\_agent.PPOAgent}}
\doxysubsubsection{\texorpdfstring{learn()}{learn()}}
{\footnotesize\ttfamily \label{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a7899457347773c0e1ef284fe4cd91d19} 
rl.\+agents.\+high\+\_\+level.\+ppo\+\_\+agent.\+PPOAgent.\+learn (\begin{DoxyParamCaption}\item[{}]{self}{}\end{DoxyParamCaption})}



Perform PPO learning update using stored experiences. 

Implements the PPO algorithm with clipped probability ratios and Generalized Advantage Estimation (GAE). Updates both actor and critic networks for multiple epochs. 
\begin{DoxyCode}{0}
\DoxyCodeLine{00266\ \ \ \ \ \textcolor{keyword}{def\ }learn(self): }
\DoxyCodeLine{00267\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}! }}
\DoxyCodeLine{00268\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @brief\ Perform\ PPO\ learning\ update\ using\ stored\ experiences }}
\DoxyCodeLine{00269\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \  }}
\DoxyCodeLine{00270\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Implements\ the\ PPO\ algorithm\ with\ clipped\ probability\ ratios\ and\  }}
\DoxyCodeLine{00271\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ Generalized\ Advantage\ Estimation\ (GAE).\ Updates\ both\ actor\ and\  }}
\DoxyCodeLine{00272\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ critic\ networks\ for\ multiple\ epochs. }}
\DoxyCodeLine{00273\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}} }
\DoxyCodeLine{00274\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ \_\ \textcolor{keywordflow}{in}\ range(self.n\_epochs): }
\DoxyCodeLine{00275\ \ \ \ \ \ \ \ \ \ \ \ \ state\_arr,\ action\_arr,\ old\_probs\_arr,\ values\_arr,\(\backslash\) }
\DoxyCodeLine{00276\ \ \ \ \ \ \ \ \ \ \ \ \ reward\_arr,\ done\_arr,\ batches\ =\ self.memory.generate\_batches() }
\DoxyCodeLine{00277\  }
\DoxyCodeLine{00278\ \ \ \ \ \ \ \ \ \ \ \ \ values\ =\ values\_arr }
\DoxyCodeLine{00279\ \ \ \ \ \ \ \ \ \ \ \ \ advantages\ =\ np.zeros(len(reward\_arr),\ dtype=np.float32) }
\DoxyCodeLine{00280\  }
\DoxyCodeLine{00281\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Calculate\ advantages\ using\ GAE }}
\DoxyCodeLine{00282\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ t\ \textcolor{keywordflow}{in}\ range(len(reward\_arr)-\/1): }
\DoxyCodeLine{00283\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ discount\ =\ 1 }
\DoxyCodeLine{00284\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ a\_t\ =\ 0 }
\DoxyCodeLine{00285\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ k\ \textcolor{keywordflow}{in}\ range(t,\ len(reward\_arr)-\/1): }
\DoxyCodeLine{00286\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ a\_t\ +=\ discount*(reward\_arr[k]\ +\ self.gamma*values[k+1]*(1-\/int(done\_arr[k]))\ -\/\ values[k]) }
\DoxyCodeLine{00287\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ discount\ *=\ self.gamma*self.gae\_lambda }
\DoxyCodeLine{00288\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ advantages[t]\ =\ a\_t }
\DoxyCodeLine{00289\  }
\DoxyCodeLine{00290\ \ \ \ \ \ \ \ \ \ \ \ \ advantage\ =\ T.tensor(advantages).to(self.actor.device) }
\DoxyCodeLine{00291\ \ \ \ \ \ \ \ \ \ \ \ \ values\ =\ T.tensor(values).to(self.actor.device) }
\DoxyCodeLine{00292\  }
\DoxyCodeLine{00293\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Train\ on\ each\ batch }}
\DoxyCodeLine{00294\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ batch\ \textcolor{keywordflow}{in}\ batches: }
\DoxyCodeLine{00295\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ states\ =\ T.tensor(state\_arr[batch],\ dtype=T.float).to(self.actor.device) }
\DoxyCodeLine{00296\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ old\_probs\ =\ T.tensor(old\_probs\_arr[batch]).to(self.actor.device) }
\DoxyCodeLine{00297\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ actions\ =\ T.tensor(action\_arr[batch]).to(self.actor.device) }
\DoxyCodeLine{00298\  }
\DoxyCodeLine{00299\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ dist\ =\ self.actor(states) }
\DoxyCodeLine{00300\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ critic\_value\ =\ self.critic(states) }
\DoxyCodeLine{00301\  }
\DoxyCodeLine{00302\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ critic\_value\ =\ T.squeeze(critic\_value) }
\DoxyCodeLine{00303\  }
\DoxyCodeLine{00304\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Calculate\ probability\ ratio\ and\ apply\ clipping }}
\DoxyCodeLine{00305\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ new\_probs\ =\ dist.log\_prob(actions) }
\DoxyCodeLine{00306\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ prob\_ratio\ =\ new\_probs.exp()\ /\ old\_probs.exp() }
\DoxyCodeLine{00307\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ weighted\_probs\ =\ advantage[batch]\ *\ prob\_ratio }
\DoxyCodeLine{00308\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ weighted\_clipped\_probs\ =\ T.clamp(prob\_ratio,\ 1-\/self.policy\_clip,\ 1+self.policy\_clip)\ *\ advantage[batch] }
\DoxyCodeLine{00309\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ actor\_loss\ =\ -\/T.min(weighted\_probs,\ weighted\_clipped\_probs).mean() }
\DoxyCodeLine{00310\  }
\DoxyCodeLine{00311\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Calculate\ critic\ loss }}
\DoxyCodeLine{00312\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ returns\ =\ advantage[batch]\ +\ values[batch] }
\DoxyCodeLine{00313\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ critic\_loss\ =\ (returns-\/critic\_value)**2 }
\DoxyCodeLine{00314\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ critic\_loss\ =\ critic\_loss.mean() }
\DoxyCodeLine{00315\  }
\DoxyCodeLine{00316\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Combined\ loss\ and\ backpropagation }}
\DoxyCodeLine{00317\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ total\_loss\ =\ actor\_loss\ +\ 0.5*critic\_loss }
\DoxyCodeLine{00318\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.actor.optimizer.zero\_grad() }
\DoxyCodeLine{00319\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.critic.optimizer.zero\_grad() }
\DoxyCodeLine{00320\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ total\_loss.backward() }
\DoxyCodeLine{00321\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.actor.optimizer.step() }
\DoxyCodeLine{00322\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ self.critic.optimizer.step() }
\DoxyCodeLine{00323\  }
\DoxyCodeLine{00324\ \ \ \ \ \ \ \ \ self.memory.clear\_memory() }
\DoxyCodeLine{00325\  }

\end{DoxyCode}
\Hypertarget{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a661abfa90d86386f305e8f947f0322f9}\index{rl.agents.high\_level.ppo\_agent.PPOAgent@{rl.agents.high\_level.ppo\_agent.PPOAgent}!load\_models@{load\_models}}
\index{load\_models@{load\_models}!rl.agents.high\_level.ppo\_agent.PPOAgent@{rl.agents.high\_level.ppo\_agent.PPOAgent}}
\doxysubsubsection{\texorpdfstring{load\_models()}{load\_models()}}
{\footnotesize\ttfamily \label{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a661abfa90d86386f305e8f947f0322f9} 
rl.\+agents.\+high\+\_\+level.\+ppo\+\_\+agent.\+PPOAgent.\+load\+\_\+models (\begin{DoxyParamCaption}\item[{}]{self}{}\end{DoxyParamCaption})}



Load both actor and critic model checkpoints. 


\begin{DoxyCode}{0}
\DoxyCodeLine{00238\ \ \ \ \ \textcolor{keyword}{def\ }load\_models(self): }
\DoxyCodeLine{00239\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}! }}
\DoxyCodeLine{00240\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @brief\ Load\ both\ actor\ and\ critic\ model\ checkpoints }}
\DoxyCodeLine{00241\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}} }
\DoxyCodeLine{00242\ \ \ \ \ \ \ \ \ print(\textcolor{stringliteral}{'...\ loading\ models\ ...'}) }
\DoxyCodeLine{00243\ \ \ \ \ \ \ \ \ self.actor.load\_checkpoint() }
\DoxyCodeLine{00244\ \ \ \ \ \ \ \ \ self.critic.load\_checkpoint() }
\DoxyCodeLine{00245\  }

\end{DoxyCode}
\Hypertarget{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a7caa3ca2d12d84e7eab133650fa115c4}\index{rl.agents.high\_level.ppo\_agent.PPOAgent@{rl.agents.high\_level.ppo\_agent.PPOAgent}!remember@{remember}}
\index{remember@{remember}!rl.agents.high\_level.ppo\_agent.PPOAgent@{rl.agents.high\_level.ppo\_agent.PPOAgent}}
\doxysubsubsection{\texorpdfstring{remember()}{remember()}}
{\footnotesize\ttfamily \label{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a7caa3ca2d12d84e7eab133650fa115c4} 
rl.\+agents.\+high\+\_\+level.\+ppo\+\_\+agent.\+PPOAgent.\+remember (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{state}{, }\item[{}]{action}{, }\item[{}]{probs}{, }\item[{}]{values}{, }\item[{}]{reward}{, }\item[{}]{done}{}\end{DoxyParamCaption})}



Store an experience in the agent\textquotesingle{}s memory. 


\begin{DoxyParams}{Parameters}
{\em state} & Current state observation \\
\hline
{\em action} & Action taken \\
\hline
{\em probs} & Log probability of the action \\
\hline
{\em values} & Value estimate for the state \\
\hline
{\em reward} & Reward received \\
\hline
{\em done} & Whether the episode is finished \\
\hline
\end{DoxyParams}

\begin{DoxyCode}{0}
\DoxyCodeLine{00218\ \ \ \ \ \textcolor{keyword}{def\ }remember(self,\ state,\ action,\ probs,\ values,\ reward,\ done): }
\DoxyCodeLine{00219\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}! }}
\DoxyCodeLine{00220\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @brief\ Store\ an\ experience\ in\ the\ agent's\ memory }}
\DoxyCodeLine{00221\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @param\ state\ Current\ state\ observation }}
\DoxyCodeLine{00222\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @param\ action\ Action\ taken }}
\DoxyCodeLine{00223\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @param\ probs\ Log\ probability\ of\ the\ action }}
\DoxyCodeLine{00224\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @param\ values\ Value\ estimate\ for\ the\ state }}
\DoxyCodeLine{00225\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @param\ reward\ Reward\ received }}
\DoxyCodeLine{00226\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @param\ done\ Whether\ the\ episode\ is\ finished }}
\DoxyCodeLine{00227\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}} }
\DoxyCodeLine{00228\ \ \ \ \ \ \ \ \ self.memory.store\_memory(state,\ action,\ probs,\ values,\ reward,\ done) }
\DoxyCodeLine{00229\  }

\end{DoxyCode}
\Hypertarget{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a470678495edd7cd31b9c58d6d95ed7c0}\index{rl.agents.high\_level.ppo\_agent.PPOAgent@{rl.agents.high\_level.ppo\_agent.PPOAgent}!save\_models@{save\_models}}
\index{save\_models@{save\_models}!rl.agents.high\_level.ppo\_agent.PPOAgent@{rl.agents.high\_level.ppo\_agent.PPOAgent}}
\doxysubsubsection{\texorpdfstring{save\_models()}{save\_models()}}
{\footnotesize\ttfamily \label{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a470678495edd7cd31b9c58d6d95ed7c0} 
rl.\+agents.\+high\+\_\+level.\+ppo\+\_\+agent.\+PPOAgent.\+save\+\_\+models (\begin{DoxyParamCaption}\item[{}]{self}{}\end{DoxyParamCaption})}



Save both actor and critic model checkpoints. 


\begin{DoxyCode}{0}
\DoxyCodeLine{00230\ \ \ \ \ \textcolor{keyword}{def\ }save\_models(self): }
\DoxyCodeLine{00231\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}! }}
\DoxyCodeLine{00232\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @brief\ Save\ both\ actor\ and\ critic\ model\ checkpoints }}
\DoxyCodeLine{00233\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}} }
\DoxyCodeLine{00234\ \ \ \ \ \ \ \ \ print(\textcolor{stringliteral}{'...\ saving\ models\ ...'}) }
\DoxyCodeLine{00235\ \ \ \ \ \ \ \ \ self.actor.save\_checkpoint() }
\DoxyCodeLine{00236\ \ \ \ \ \ \ \ \ self.critic.save\_checkpoint() }
\DoxyCodeLine{00237\  }

\end{DoxyCode}


\doxysubsection{Member Data Documentation}
\Hypertarget{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a30d5505efa99710341c579de4f3c7c03}\index{rl.agents.high\_level.ppo\_agent.PPOAgent@{rl.agents.high\_level.ppo\_agent.PPOAgent}!actor@{actor}}
\index{actor@{actor}!rl.agents.high\_level.ppo\_agent.PPOAgent@{rl.agents.high\_level.ppo\_agent.PPOAgent}}
\doxysubsubsection{\texorpdfstring{actor}{actor}}
{\footnotesize\ttfamily \label{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a30d5505efa99710341c579de4f3c7c03} 
rl.\+agents.\+high\+\_\+level.\+ppo\+\_\+agent.\+PPOAgent.\+actor = \mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_actor_network}{Actor\+Network}}(n\+\_\+actions, input\+\_\+dims, alpha, name = model\+\_\+name)}

\Hypertarget{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a476d79433aee6877dcf08d4985a8b798}\index{rl.agents.high\_level.ppo\_agent.PPOAgent@{rl.agents.high\_level.ppo\_agent.PPOAgent}!critic@{critic}}
\index{critic@{critic}!rl.agents.high\_level.ppo\_agent.PPOAgent@{rl.agents.high\_level.ppo\_agent.PPOAgent}}
\doxysubsubsection{\texorpdfstring{critic}{critic}}
{\footnotesize\ttfamily \label{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a476d79433aee6877dcf08d4985a8b798} 
rl.\+agents.\+high\+\_\+level.\+ppo\+\_\+agent.\+PPOAgent.\+critic = \mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_critic_network}{Critic\+Network}}(input\+\_\+dims, alpha, name = model\+\_\+name)}

\Hypertarget{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a2e14dfc5da07b12391625092aad3a8da}\index{rl.agents.high\_level.ppo\_agent.PPOAgent@{rl.agents.high\_level.ppo\_agent.PPOAgent}!gae\_lambda@{gae\_lambda}}
\index{gae\_lambda@{gae\_lambda}!rl.agents.high\_level.ppo\_agent.PPOAgent@{rl.agents.high\_level.ppo\_agent.PPOAgent}}
\doxysubsubsection{\texorpdfstring{gae\_lambda}{gae\_lambda}}
{\footnotesize\ttfamily \label{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a2e14dfc5da07b12391625092aad3a8da} 
rl.\+agents.\+high\+\_\+level.\+ppo\+\_\+agent.\+PPOAgent.\+gae\+\_\+lambda = gae\+\_\+lambda}

\Hypertarget{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a9a01dc3182b551836c5549a8c172b651}\index{rl.agents.high\_level.ppo\_agent.PPOAgent@{rl.agents.high\_level.ppo\_agent.PPOAgent}!gamma@{gamma}}
\index{gamma@{gamma}!rl.agents.high\_level.ppo\_agent.PPOAgent@{rl.agents.high\_level.ppo\_agent.PPOAgent}}
\doxysubsubsection{\texorpdfstring{gamma}{gamma}}
{\footnotesize\ttfamily \label{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a9a01dc3182b551836c5549a8c172b651} 
rl.\+agents.\+high\+\_\+level.\+ppo\+\_\+agent.\+PPOAgent.\+gamma = gamma}

\Hypertarget{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_aa806d28253ea490fc8b519af229bec28}\index{rl.agents.high\_level.ppo\_agent.PPOAgent@{rl.agents.high\_level.ppo\_agent.PPOAgent}!memory@{memory}}
\index{memory@{memory}!rl.agents.high\_level.ppo\_agent.PPOAgent@{rl.agents.high\_level.ppo\_agent.PPOAgent}}
\doxysubsubsection{\texorpdfstring{memory}{memory}}
{\footnotesize\ttfamily \label{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_aa806d28253ea490fc8b519af229bec28} 
rl.\+agents.\+high\+\_\+level.\+ppo\+\_\+agent.\+PPOAgent.\+memory = \mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory}{PPOMemory}}(batch\+\_\+size)}

\Hypertarget{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a7ba79036643adb93f11d205be1dd0b5f}\index{rl.agents.high\_level.ppo\_agent.PPOAgent@{rl.agents.high\_level.ppo\_agent.PPOAgent}!n\_epochs@{n\_epochs}}
\index{n\_epochs@{n\_epochs}!rl.agents.high\_level.ppo\_agent.PPOAgent@{rl.agents.high\_level.ppo\_agent.PPOAgent}}
\doxysubsubsection{\texorpdfstring{n\_epochs}{n\_epochs}}
{\footnotesize\ttfamily \label{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a7ba79036643adb93f11d205be1dd0b5f} 
rl.\+agents.\+high\+\_\+level.\+ppo\+\_\+agent.\+PPOAgent.\+n\+\_\+epochs = n\+\_\+epochs}

\Hypertarget{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a6943862574b28a3371d19ee56455a544}\index{rl.agents.high\_level.ppo\_agent.PPOAgent@{rl.agents.high\_level.ppo\_agent.PPOAgent}!policy\_clip@{policy\_clip}}
\index{policy\_clip@{policy\_clip}!rl.agents.high\_level.ppo\_agent.PPOAgent@{rl.agents.high\_level.ppo\_agent.PPOAgent}}
\doxysubsubsection{\texorpdfstring{policy\_clip}{policy\_clip}}
{\footnotesize\ttfamily \label{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_agent_a6943862574b28a3371d19ee56455a544} 
rl.\+agents.\+high\+\_\+level.\+ppo\+\_\+agent.\+PPOAgent.\+policy\+\_\+clip = policy\+\_\+clip}



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
rl/agents/high\+\_\+level/ppo\+\_\+agent.\+py\end{DoxyCompactItemize}
