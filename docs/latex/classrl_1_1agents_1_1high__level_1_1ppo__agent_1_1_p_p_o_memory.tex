\doxysection{rl.\+agents.\+high\+\_\+level.\+ppo\+\_\+agent.\+PPOMemory Class Reference}
\hypertarget{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory}{}\label{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory}\index{rl.agents.high\_level.ppo\_agent.PPOMemory@{rl.agents.high\_level.ppo\_agent.PPOMemory}}


Memory buffer for storing PPO training experiences.  


\doxysubsubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_a4be12d895d6566a136a7e7d3a437da6e}{\+\_\+\+\_\+init\+\_\+\+\_\+}} (self, \mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_a0c97a17d1a3dd1bcfbe8bc82655dee63}{batch\+\_\+size}})
\begin{DoxyCompactList}\small\item\em Initialize the PPO memory buffer. \end{DoxyCompactList}\item 
\mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_a4a87d310ee11c3ce1d58da34eb7b7d03}{generate\+\_\+batches}} (self)
\begin{DoxyCompactList}\small\item\em Generate randomized training batches from stored experiences. \end{DoxyCompactList}\item 
\mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_acb69a1ceca60755d44ff61f0b7eb4d93}{store\+\_\+memory}} (self, state, action, \mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_aba4f59ce24a7a1e276c121cf9fff1512}{probs}}, \mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_a47453ce0abdb11c30dd9f8222a96b457}{values}}, reward, done)
\begin{DoxyCompactList}\small\item\em Store a single experience in the memory buffer. \end{DoxyCompactList}\item 
\mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_a0b27fdd1b6d3d971bc034b616b58f7d7}{clear\+\_\+memory}} (self)
\begin{DoxyCompactList}\small\item\em Clear all stored experiences from memory. \end{DoxyCompactList}\end{DoxyCompactItemize}
\doxysubsubsection*{Public Attributes}
\begin{DoxyCompactItemize}
\item 
list \mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_a0add6132e8c45fde1017f3d1b9fd62e1}{states}} = \mbox{[}$\,$\mbox{]}
\item 
list \mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_aba4f59ce24a7a1e276c121cf9fff1512}{probs}} = \mbox{[}$\,$\mbox{]}
\item 
list \mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_a47453ce0abdb11c30dd9f8222a96b457}{values}} = \mbox{[}$\,$\mbox{]}
\item 
list \mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_a3bf4561f4eb99d3693f9128ae0522b69}{actions}} = \mbox{[}$\,$\mbox{]}
\item 
list \mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_acf933f33d5540126ed764ff35e0667ef}{rewards}} = \mbox{[}$\,$\mbox{]}
\item 
list \mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_a0e9efad74c28d7fd7e7bd0fda0257cac}{dones}} = \mbox{[}$\,$\mbox{]}
\item 
\mbox{\hyperlink{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_a0c97a17d1a3dd1bcfbe8bc82655dee63}{batch\+\_\+size}} = batch\+\_\+size
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
Memory buffer for storing PPO training experiences. 

This class manages the storage and retrieval of experiences for PPO training, including states, actions, probabilities, values, rewards, and done flags. 

\doxysubsection{Constructor \& Destructor Documentation}
\Hypertarget{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_a4be12d895d6566a136a7e7d3a437da6e}\index{rl.agents.high\_level.ppo\_agent.PPOMemory@{rl.agents.high\_level.ppo\_agent.PPOMemory}!\_\_init\_\_@{\_\_init\_\_}}
\index{\_\_init\_\_@{\_\_init\_\_}!rl.agents.high\_level.ppo\_agent.PPOMemory@{rl.agents.high\_level.ppo\_agent.PPOMemory}}
\doxysubsubsection{\texorpdfstring{\_\_init\_\_()}{\_\_init\_\_()}}
{\footnotesize\ttfamily \label{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_a4be12d895d6566a136a7e7d3a437da6e} 
rl.\+agents.\+high\+\_\+level.\+ppo\+\_\+agent.\+PPOMemory.\+\_\+\+\_\+init\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{batch\+\_\+size}{}\end{DoxyParamCaption})}



Initialize the PPO memory buffer. 


\begin{DoxyParams}{Parameters}
{\em batch\+\_\+size} & Size of batches for training \\
\hline
\end{DoxyParams}

\begin{DoxyCode}{0}
\DoxyCodeLine{00016\ \ \ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(self,\ batch\_size): }
\DoxyCodeLine{00017\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}! }}
\DoxyCodeLine{00018\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @brief\ Initialize\ the\ PPO\ memory\ buffer }}
\DoxyCodeLine{00019\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @param\ batch\_size\ Size\ of\ batches\ for\ training }}
\DoxyCodeLine{00020\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}} }
\DoxyCodeLine{00021\ \ \ \ \ \ \ \ \ self.states\ =\ [] }
\DoxyCodeLine{00022\ \ \ \ \ \ \ \ \ self.probs\ =\ [] }
\DoxyCodeLine{00023\ \ \ \ \ \ \ \ \ self.values\ =\ [] }
\DoxyCodeLine{00024\ \ \ \ \ \ \ \ \ self.actions\ =\ [] }
\DoxyCodeLine{00025\ \ \ \ \ \ \ \ \ self.rewards\ =\ [] }
\DoxyCodeLine{00026\ \ \ \ \ \ \ \ \ self.dones\ =\ [] }
\DoxyCodeLine{00027\  }
\DoxyCodeLine{00028\ \ \ \ \ \ \ \ \ self.batch\_size\ =\ batch\_size }
\DoxyCodeLine{00029\  }

\end{DoxyCode}


\doxysubsection{Member Function Documentation}
\Hypertarget{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_a0b27fdd1b6d3d971bc034b616b58f7d7}\index{rl.agents.high\_level.ppo\_agent.PPOMemory@{rl.agents.high\_level.ppo\_agent.PPOMemory}!clear\_memory@{clear\_memory}}
\index{clear\_memory@{clear\_memory}!rl.agents.high\_level.ppo\_agent.PPOMemory@{rl.agents.high\_level.ppo\_agent.PPOMemory}}
\doxysubsubsection{\texorpdfstring{clear\_memory()}{clear\_memory()}}
{\footnotesize\ttfamily \label{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_a0b27fdd1b6d3d971bc034b616b58f7d7} 
rl.\+agents.\+high\+\_\+level.\+ppo\+\_\+agent.\+PPOMemory.\+clear\+\_\+memory (\begin{DoxyParamCaption}\item[{}]{self}{}\end{DoxyParamCaption})}



Clear all stored experiences from memory. 


\begin{DoxyCode}{0}
\DoxyCodeLine{00063\ \ \ \ \ \textcolor{keyword}{def\ }clear\_memory(self): }
\DoxyCodeLine{00064\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}! }}
\DoxyCodeLine{00065\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @brief\ Clear\ all\ stored\ experiences\ from\ memory }}
\DoxyCodeLine{00066\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}} }
\DoxyCodeLine{00067\ \ \ \ \ \ \ \ \ self.states\ =\ [] }
\DoxyCodeLine{00068\ \ \ \ \ \ \ \ \ self.actions\ =\ [] }
\DoxyCodeLine{00069\ \ \ \ \ \ \ \ \ self.probs\ =\ [] }
\DoxyCodeLine{00070\ \ \ \ \ \ \ \ \ self.values\ =\ [] }
\DoxyCodeLine{00071\ \ \ \ \ \ \ \ \ self.rewards\ =\ [] }
\DoxyCodeLine{00072\ \ \ \ \ \ \ \ \ self.dones\ =\ [] }
\DoxyCodeLine{00073\  }
\DoxyCodeLine{00074\  }

\end{DoxyCode}
\Hypertarget{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_a4a87d310ee11c3ce1d58da34eb7b7d03}\index{rl.agents.high\_level.ppo\_agent.PPOMemory@{rl.agents.high\_level.ppo\_agent.PPOMemory}!generate\_batches@{generate\_batches}}
\index{generate\_batches@{generate\_batches}!rl.agents.high\_level.ppo\_agent.PPOMemory@{rl.agents.high\_level.ppo\_agent.PPOMemory}}
\doxysubsubsection{\texorpdfstring{generate\_batches()}{generate\_batches()}}
{\footnotesize\ttfamily \label{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_a4a87d310ee11c3ce1d58da34eb7b7d03} 
rl.\+agents.\+high\+\_\+level.\+ppo\+\_\+agent.\+PPOMemory.\+generate\+\_\+batches (\begin{DoxyParamCaption}\item[{}]{self}{}\end{DoxyParamCaption})}



Generate randomized training batches from stored experiences. 

\begin{DoxyReturn}{Returns}
Tuple containing arrays of states, actions, probabilities, values, rewards, dones, and batch indices 
\end{DoxyReturn}

\begin{DoxyCode}{0}
\DoxyCodeLine{00030\ \ \ \ \ \textcolor{keyword}{def\ }generate\_batches(self): }
\DoxyCodeLine{00031\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}! }}
\DoxyCodeLine{00032\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @brief\ Generate\ randomized\ training\ batches\ from\ stored\ experiences }}
\DoxyCodeLine{00033\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @return\ Tuple\ containing\ arrays\ of\ states,\ actions,\ probabilities,\ values,\ rewards,\ dones,\ and\ batch\ indices }}
\DoxyCodeLine{00034\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}} }
\DoxyCodeLine{00035\ \ \ \ \ \ \ \ \ n\_states\ =\ len(self.states) }
\DoxyCodeLine{00036\ \ \ \ \ \ \ \ \ batch\_start\ =\ np.arange(0,\ n\_states,\ self.batch\_size) }
\DoxyCodeLine{00037\ \ \ \ \ \ \ \ \ indices\ =\ np.arange(n\_states,\ dtype=np.int64) }
\DoxyCodeLine{00038\ \ \ \ \ \ \ \ \ np.random.shuffle(indices) }
\DoxyCodeLine{00039\ \ \ \ \ \ \ \ \ batches\ =\ [indices[i:i+self.batch\_size]\ \textcolor{keywordflow}{for}\ i\ \textcolor{keywordflow}{in}\ batch\_start] }
\DoxyCodeLine{00040\  }
\DoxyCodeLine{00041\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ np.array(self.states),\ np.array(self.actions),\(\backslash\) }
\DoxyCodeLine{00042\ \ \ \ \ \ \ \ \ \ \ \ \ np.array(self.probs),\ np.array(self.values),\(\backslash\) }
\DoxyCodeLine{00043\ \ \ \ \ \ \ \ \ \ \ \ \ np.array(self.rewards),\ np.array(self.dones),\(\backslash\) }
\DoxyCodeLine{00044\ \ \ \ \ \ \ \ \ \ \ \ \ batches }
\DoxyCodeLine{00045\  }

\end{DoxyCode}
\Hypertarget{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_acb69a1ceca60755d44ff61f0b7eb4d93}\index{rl.agents.high\_level.ppo\_agent.PPOMemory@{rl.agents.high\_level.ppo\_agent.PPOMemory}!store\_memory@{store\_memory}}
\index{store\_memory@{store\_memory}!rl.agents.high\_level.ppo\_agent.PPOMemory@{rl.agents.high\_level.ppo\_agent.PPOMemory}}
\doxysubsubsection{\texorpdfstring{store\_memory()}{store\_memory()}}
{\footnotesize\ttfamily \label{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_acb69a1ceca60755d44ff61f0b7eb4d93} 
rl.\+agents.\+high\+\_\+level.\+ppo\+\_\+agent.\+PPOMemory.\+store\+\_\+memory (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{}]{state}{, }\item[{}]{action}{, }\item[{}]{probs}{, }\item[{}]{values}{, }\item[{}]{reward}{, }\item[{}]{done}{}\end{DoxyParamCaption})}



Store a single experience in the memory buffer. 


\begin{DoxyParams}{Parameters}
{\em state} & Current state observation \\
\hline
{\em action} & Action taken \\
\hline
{\em probs} & Action probability from policy \\
\hline
{\em values} & State value estimate \\
\hline
{\em reward} & Received reward \\
\hline
{\em done} & Episode termination flag \\
\hline
\end{DoxyParams}

\begin{DoxyCode}{0}
\DoxyCodeLine{00046\ \ \ \ \ \textcolor{keyword}{def\ }store\_memory(self,\ state,\ action,\ probs,\ values,\ reward,\ done): }
\DoxyCodeLine{00047\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}! }}
\DoxyCodeLine{00048\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @brief\ Store\ a\ single\ experience\ in\ the\ memory\ buffer }}
\DoxyCodeLine{00049\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @param\ state\ Current\ state\ observation }}
\DoxyCodeLine{00050\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @param\ action\ Action\ taken }}
\DoxyCodeLine{00051\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @param\ probs\ Action\ probability\ from\ policy }}
\DoxyCodeLine{00052\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @param\ values\ State\ value\ estimate }}
\DoxyCodeLine{00053\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @param\ reward\ Received\ reward }}
\DoxyCodeLine{00054\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ @param\ done\ Episode\ termination\ flag }}
\DoxyCodeLine{00055\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ "{}"{}"{}} }
\DoxyCodeLine{00056\ \ \ \ \ \ \ \ \ self.states.append(state) }
\DoxyCodeLine{00057\ \ \ \ \ \ \ \ \ self.actions.append(action) }
\DoxyCodeLine{00058\ \ \ \ \ \ \ \ \ self.probs.append(probs) }
\DoxyCodeLine{00059\ \ \ \ \ \ \ \ \ self.values.append(values) }
\DoxyCodeLine{00060\ \ \ \ \ \ \ \ \ self.rewards.append(reward) }
\DoxyCodeLine{00061\ \ \ \ \ \ \ \ \ self.dones.append(done) }
\DoxyCodeLine{00062\  }

\end{DoxyCode}


\doxysubsection{Member Data Documentation}
\Hypertarget{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_a3bf4561f4eb99d3693f9128ae0522b69}\index{rl.agents.high\_level.ppo\_agent.PPOMemory@{rl.agents.high\_level.ppo\_agent.PPOMemory}!actions@{actions}}
\index{actions@{actions}!rl.agents.high\_level.ppo\_agent.PPOMemory@{rl.agents.high\_level.ppo\_agent.PPOMemory}}
\doxysubsubsection{\texorpdfstring{actions}{actions}}
{\footnotesize\ttfamily \label{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_a3bf4561f4eb99d3693f9128ae0522b69} 
list rl.\+agents.\+high\+\_\+level.\+ppo\+\_\+agent.\+PPOMemory.\+actions = \mbox{[}$\,$\mbox{]}}

\Hypertarget{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_a0c97a17d1a3dd1bcfbe8bc82655dee63}\index{rl.agents.high\_level.ppo\_agent.PPOMemory@{rl.agents.high\_level.ppo\_agent.PPOMemory}!batch\_size@{batch\_size}}
\index{batch\_size@{batch\_size}!rl.agents.high\_level.ppo\_agent.PPOMemory@{rl.agents.high\_level.ppo\_agent.PPOMemory}}
\doxysubsubsection{\texorpdfstring{batch\_size}{batch\_size}}
{\footnotesize\ttfamily \label{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_a0c97a17d1a3dd1bcfbe8bc82655dee63} 
rl.\+agents.\+high\+\_\+level.\+ppo\+\_\+agent.\+PPOMemory.\+batch\+\_\+size = batch\+\_\+size}

\Hypertarget{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_a0e9efad74c28d7fd7e7bd0fda0257cac}\index{rl.agents.high\_level.ppo\_agent.PPOMemory@{rl.agents.high\_level.ppo\_agent.PPOMemory}!dones@{dones}}
\index{dones@{dones}!rl.agents.high\_level.ppo\_agent.PPOMemory@{rl.agents.high\_level.ppo\_agent.PPOMemory}}
\doxysubsubsection{\texorpdfstring{dones}{dones}}
{\footnotesize\ttfamily \label{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_a0e9efad74c28d7fd7e7bd0fda0257cac} 
list rl.\+agents.\+high\+\_\+level.\+ppo\+\_\+agent.\+PPOMemory.\+dones = \mbox{[}$\,$\mbox{]}}

\Hypertarget{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_aba4f59ce24a7a1e276c121cf9fff1512}\index{rl.agents.high\_level.ppo\_agent.PPOMemory@{rl.agents.high\_level.ppo\_agent.PPOMemory}!probs@{probs}}
\index{probs@{probs}!rl.agents.high\_level.ppo\_agent.PPOMemory@{rl.agents.high\_level.ppo\_agent.PPOMemory}}
\doxysubsubsection{\texorpdfstring{probs}{probs}}
{\footnotesize\ttfamily \label{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_aba4f59ce24a7a1e276c121cf9fff1512} 
list rl.\+agents.\+high\+\_\+level.\+ppo\+\_\+agent.\+PPOMemory.\+probs = \mbox{[}$\,$\mbox{]}}

\Hypertarget{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_acf933f33d5540126ed764ff35e0667ef}\index{rl.agents.high\_level.ppo\_agent.PPOMemory@{rl.agents.high\_level.ppo\_agent.PPOMemory}!rewards@{rewards}}
\index{rewards@{rewards}!rl.agents.high\_level.ppo\_agent.PPOMemory@{rl.agents.high\_level.ppo\_agent.PPOMemory}}
\doxysubsubsection{\texorpdfstring{rewards}{rewards}}
{\footnotesize\ttfamily \label{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_acf933f33d5540126ed764ff35e0667ef} 
list rl.\+agents.\+high\+\_\+level.\+ppo\+\_\+agent.\+PPOMemory.\+rewards = \mbox{[}$\,$\mbox{]}}

\Hypertarget{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_a0add6132e8c45fde1017f3d1b9fd62e1}\index{rl.agents.high\_level.ppo\_agent.PPOMemory@{rl.agents.high\_level.ppo\_agent.PPOMemory}!states@{states}}
\index{states@{states}!rl.agents.high\_level.ppo\_agent.PPOMemory@{rl.agents.high\_level.ppo\_agent.PPOMemory}}
\doxysubsubsection{\texorpdfstring{states}{states}}
{\footnotesize\ttfamily \label{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_a0add6132e8c45fde1017f3d1b9fd62e1} 
list rl.\+agents.\+high\+\_\+level.\+ppo\+\_\+agent.\+PPOMemory.\+states = \mbox{[}$\,$\mbox{]}}

\Hypertarget{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_a47453ce0abdb11c30dd9f8222a96b457}\index{rl.agents.high\_level.ppo\_agent.PPOMemory@{rl.agents.high\_level.ppo\_agent.PPOMemory}!values@{values}}
\index{values@{values}!rl.agents.high\_level.ppo\_agent.PPOMemory@{rl.agents.high\_level.ppo\_agent.PPOMemory}}
\doxysubsubsection{\texorpdfstring{values}{values}}
{\footnotesize\ttfamily \label{classrl_1_1agents_1_1high__level_1_1ppo__agent_1_1_p_p_o_memory_a47453ce0abdb11c30dd9f8222a96b457} 
list rl.\+agents.\+high\+\_\+level.\+ppo\+\_\+agent.\+PPOMemory.\+values = \mbox{[}$\,$\mbox{]}}



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
rl/agents/high\+\_\+level/ppo\+\_\+agent.\+py\end{DoxyCompactItemize}
