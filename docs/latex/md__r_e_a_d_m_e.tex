\chapter{Architektur√ºbersicht\+: Hierarchischer RL-\/\+Agent f√ºr die Beluga Challenge}
\hypertarget{md__r_e_a_d_m_e}{}\label{md__r_e_a_d_m_e}\index{Architektur√ºbersicht: Hierarchischer RL-\/Agent f√ºr die Beluga Challenge@{Architektur√ºbersicht: Hierarchischer RL-\/Agent f√ºr die Beluga Challenge}}
\label{md__r_e_a_d_m_e_autotoc_md0}%
\Hypertarget{md__r_e_a_d_m_e_autotoc_md0}%


Unsere Architektur basiert auf einem hierarchischen Entscheidungsansatz. Der High-\/\+Level-\/\+Agent trifft strategische Entscheidungen, w√§hrend die Parametrisierung und Ausf√ºhrung √ºber spezialisierte Low-\/\+Level-\/\+Mechanismen erfolgt\+: \hypertarget{md__r_e_a_d_m_e_autotoc_md1}{}\doxysection{\texorpdfstring{Komponenten}{Komponenten}}\label{md__r_e_a_d_m_e_autotoc_md1}

\begin{DoxyItemize}
\item {\bfseries{High-\/\+Level Agent}} ~\newline
 W√§hlt eine von acht m√∂glichen Aktionen aus (z.\+‚ÄØB. {\itshape Load Jig}, {\itshape Swap}, {\itshape Dispatch}). Trainiert mit Proximal Policy Optimization (PPO).
\item {\bfseries{Low-\/\+Level Agent}} ~\newline
 Verfeinert und f√ºhrt die vom High-\/\+Level-\/\+Agenten gew√§hlte Aktion aus. Abh√§ngig von der Aktionsart geschieht dies durch\+:
\begin{DoxyItemize}
\item üîπ {\itshape Direkte Ausf√ºhrung} (z.\+‚ÄØB. deterministisch l√∂sbare Aktionen ohne Parameter)
\item üîπ {\itshape Heuristiken} (f√ºr einfache, aber parametrisierte Aktionen)
\item üîπ {\itshape Monte Carlo Tree Search (MCTS)} (f√ºr komplexe, sequenzielle Entscheidungen mit hohem Kombinationsraum)
\end{DoxyItemize}
\end{DoxyItemize}

Diese modulare Trennung erlaubt es, unterschiedliche Ans√§tze (RL, Heuristiken, MCTS) synergetisch zu kombinieren und gezielt auf die Charakteristika einzelner Teilprobleme anzuwenden. 