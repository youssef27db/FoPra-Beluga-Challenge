# FoPra â€“ Beluga Challenge  
**Hierarchischer Reinforcement-Learning-Agent**

---

## ğŸ“Œ Projektbeschreibung
Dieses Projekt implementiert einen **hierarchischen RL-Agenten** fÃ¼r die *Beluga Challenge*.  
Der Ansatz trennt strategische Entscheidungen (High-Level) von der konkreten AusfÃ¼hrung (Low-Level), um unterschiedliche Methoden wie **PPO**, **Heuristiken** und **MCTS** gezielt zu kombinieren.

---

## ğŸ— ArchitekturÃ¼bersicht

Unsere Architektur folgt einem **hierarchischen Entscheidungsansatz**:

![Agent Architecture](docs/architektur.jpg)

- **High-Level Agent**  
  - WÃ¤hlt eine von acht mÃ¶glichen Aktionen (z. B. *Load Jig*, *Swap*, *Dispatch*)  
  - Trainiert mit **Proximal Policy Optimization (PPO)**  
  - Verantwortlich fÃ¼r die strategische Richtung

- **Low-Level Agent**  
  - Verfeinert und fÃ¼hrt die gewÃ¤hlte High-Level-Aktion aus  
  - Setzt je nach KomplexitÃ¤t unterschiedliche Mechanismen ein:  
    - ğŸ”¹ **Direkte AusfÃ¼hrung** â€“ deterministische Aktionen ohne Parameter  
    - ğŸ”¹ **Heuristiken** â€“ einfache parametrisierte Aktionen  
    - ğŸ”¹ **Monte Carlo Tree Search (MCTS)** â€“ komplexe, sequenzielle Entscheidungen mit groÃŸem Kombinationsraum

> ğŸ’¡ **Vorteil:** Durch diese modulare Trennung kÃ¶nnen wir die StÃ¤rken verschiedener Methoden gezielt nutzen und die Charakteristika einzelner Teilprobleme optimal abdecken.

## ğŸ“„ Projektdokumentation
Die vollstÃ¤ndige generierte HTML-Dokumentation findest du hier:  
[ğŸ“– Projekt-Dokumentation anzeigen](https://youssef27db.github.io/FoPra-Beluga-Challenge)


