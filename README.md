# FoPra â€“ Beluga Challenge  
**Hierarchischer Reinforcement-Learning-Agent**

---

## ğŸ“Œ Projektbeschreibung
Dieses Projekt implementiert einen **hierarchischen RL-Agenten** fÃ¼r die *Beluga Challenge*.  
Der Ansatz trennt strategische Entscheidungen (High-Level) von der konkreten AusfÃ¼hrung (Low-Level), um unterschiedliche Methoden wie **PPO**, **Heuristiken** und **MCTS** gezielt zu kombinieren.

---

## ğŸ— ArchitekturÃ¼bersicht

Unsere Architektur folgt einem **hierarchischen Entscheidungsansatz**:

![Agent Architecture](https://github.com/user-attachments/assets/ac2d5b83-8f99-4fbd-b97b-1441114ee30b)

- **High-Level Agent**  
  - WÃ¤hlt eine von acht mÃ¶glichen Aktionen (z. B. *Load Jig*, *Swap*, *Dispatch*)  
  - Trainiert mit **Proximal Policy Optimization (PPO)**  
  - Verantwortlich fÃ¼r die strategische Richtung

- **Low-Level Agent**  
  - Verfeinert und fÃ¼hrt die gewÃ¤hlte High-Level-Aktion aus  
  - Setzt je nach KomplexitÃ¤t unterschiedliche Mechanismen ein:  
    - ğŸ”¹ **Direkte AusfÃ¼hrung** â€“ deterministische Aktionen ohne Parameter  
    - ğŸ”¹ **Heuristiken** â€“ einfache parametrisierte Aktionen  
    - ğŸ”¹ **Monte Carlo Tree Search (MCTS)** â€“ komplexe, sequenzielle Entscheidungen mit groÃŸem Kombinationsraum

> ğŸ’¡ **Vorteil:** Durch diese modulare Trennung kÃ¶nnen wir die StÃ¤rken verschiedener Methoden gezielt nutzen und die Charakteristika einzelner Teilprobleme optimal abdecken.

## ğŸ“„ Projektdokumentation
Die vollstÃ¤ndige, generierte HTML-Dokumentation befindet sich im Ordner [`docs/html`](./docs/html).  
Du kannst sie lokal Ã¶ffnen Ã¼ber:

[**ğŸ“– Projekt-Dokumentation anzeigen**](./docs/html/index.html)

